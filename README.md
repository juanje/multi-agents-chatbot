# Multi-Agents Chatbot

A learning project to understand LangGraph workflows and agent creation. This project includes two implementations:

1. **`non-llm-chatbot.py`**: Multi-agent system without LLM connections, focusing on workflow design
2. **`llm-chatbot.py`**: Multi-agent system with LLM integration using Ollama (llama3.2:3b)

## ğŸ—ï¸ System Architecture

The system uses an agent workflow with conditional routing:

```mermaid
graph TD;
	start([START])
	chatbot["ğŸ¤– Chatbot Agent<br/>Receives user input"]
	router{"ğŸ”€ should_exit<br/>Is it exit?"}
	analysis["ğŸ” Analysis Agent<br/>Classifies message type"]
	final_answer["ğŸ’¬ Final Answer Agent<br/>Generates response"]
	finish([END])
	
	start --> chatbot
	chatbot --> router
	router -->|yes| finish
	router -->|no| analysis
	analysis --> final_answer
	final_answer --> chatbot
```

## ğŸ¤– System Components

### Agents
- **`chatbot_agent`**: Handles user input and controls conversation flow
- **`analysis_agent`**: Classifies message type (greeting, question, other)
- **`final_answer_agent`**: Generates appropriate response based on message type

### Conditional Router
- **`should_exit`**: Determines if the user wants to end the conversation

## ğŸ”„ Workflow

1. **START** â†’ `chatbot_agent` (receives user input)
2. **`chatbot_agent`** â†’ `should_exit` (routing function)
3. **`should_exit`** evaluates:
   - **If "exit"** â†’ **END** (terminates conversation)
   - **If not** â†’ continues to `analysis_agent`
4. **`analysis_agent`** â†’ Classifies message type (greeting/question/other)
5. **`final_answer_agent`** â†’ Generates appropriate response based on type
6. **Loop** â†’ Returns to `chatbot_agent` to continue conversation

## ğŸš€ Usage

### Option 1: Non-LLM Chatbot (Learning LangGraph Fundamentals)

```bash
# Install dependencies
uv sync

# Run the non-LLM chatbot
uv run python non-llm-chatbot.py
```

### Option 2: LLM-Powered Chatbot (with Ollama)

**Prerequisites:**
- Install [Ollama](https://ollama.ai/)
- Pull the model: `ollama pull llama3.2:3b`
- Make sure Ollama is running: `ollama serve`

```bash
# Install dependencies
uv sync

# Run the LLM chatbot
uv run python llm-chatbot.py
```

## ğŸ“ Supported Message Types

### Non-LLM Chatbot (`non-llm-chatbot.py`)
- **Greeting**: "hello", "hi", "hola" â†’ Predefined greeting response
- **Question**: Text ending with "?" â†’ Predefined "I don't know" response
- **Other**: Any other text â†’ Predefined generic response

### LLM Chatbot (`llm-chatbot.py`)
- **Greeting**: Detected by LLM with structured output â†’ Contextual warm greeting
- **Question**: Detected by LLM with structured output â†’ Intelligent answer generated by LLM
  - Uses detailed prompts with examples (?, who/what/where/when/why/how patterns)
- **Other**: Any other text â†’ Contextual response from LLM

**Technical Features:**
- Uses `with_structured_output()` with Pydantic for reliable classification
- Temperature 0.0 for classification (deterministic)
- Temperature 0.7 for responses (creative)
- Stores both user messages (`HumanMessage`) and bot responses (`AIMessage`) in history
- Prompts are customized based on message type for better context

## ğŸ¯ Learning Objectives

This project focuses on understanding:
- **LangGraph workflows**: How to create and connect agent nodes
- **Conditional routing**: Implementing decision logic in workflows
- **Agent design**: Creating specialized agents with single responsibilities
- **State management**: Handling data flow between agents
- **Workflow orchestration**: Building circular and conditional flows

## ğŸ› ï¸ Technologies

- **Python 3.13+**
- **LangGraph**: Framework for agent workflows
- **LangChain**: Base for message system and model integrations
- **LangChain-Ollama**: Integration with Ollama models
- **Ollama**: Local LLM runtime (llama3.2:3b)
- **uv**: Package manager

## ğŸ“š Two Learning Paths

### Path 1: `non-llm-chatbot.py` - LangGraph Fundamentals
Focus on understanding LangGraph without the complexity of LLMs:
- Pure workflow design patterns
- Conditional routing logic
- State management
- Agent orchestration
- Deterministic agent behavior

### Path 2: `llm-chatbot.py` - LLM Integration
Build upon Path 1 by adding intelligent responses:
- Integrating ChatOllama with LangGraph nodes
- LLM-powered message classification with **structured output**
- Context-aware response generation
- Dual temperature strategy (0.0 for classification, 0.7 for responses)
- Managing conversation history
- Trusting the LLM (no fallbacks for educational clarity)

## ğŸ“Š Features

### Common to Both Implementations
- âœ… **Conditional routing** for controlled termination
- âœ… **Robust type handling** with TypedDict
- âœ… **Circular workflow** for continuous conversations
- âœ… **Clean termination** with "exit" command
- âœ… **Simple, explicit code** for learning LangGraph concepts

### Non-LLM Chatbot Specific
- âœ… **No LLM dependencies** - pure workflow focus
- âœ… **Regex-based classification** - deterministic behavior
- âœ… **Predefined responses** - fast and predictable

### LLM Chatbot Specific
- âœ… **Structured output classification** - guaranteed valid types with Pydantic
- âœ… **Dual temperature strategy** - 0.0 for classification, 0.7 for responses
- âœ… **Detailed classification prompts** - includes examples and patterns for better accuracy
- âœ… **Dynamic response generation** - contextual and natural responses
- âœ… **Complete conversation history** - stores both user and assistant messages
- âœ… **Context-aware prompts** - different system prompts based on message type
- âœ… **Simplified code** - trusts the LLM without fallbacks (educational)
- âœ… **Local LLM** - privacy-focused with Ollama

## ğŸ“ Educational Value

This project is designed for progressive learning:

### Stage 1: `non-llm-chatbot.py`
- **Explicit agent functions** that are easy to understand
- **Clear workflow patterns** for agent orchestration
- **Simple state management** without complex LLM interactions
- **Conditional routing examples** for decision-making in workflows
- **Foundation** for understanding LangGraph architecture

### Stage 2: `llm-chatbot.py`
- **LLM integration** within LangGraph nodes
- **Structured output** with Pydantic for reliable classification
- **Prompt engineering** for accurate classification with detailed instructions
- **Temperature control** for different agent behaviors (0.0 vs 0.7)
- **Context management** with full conversation history (HumanMessage + AIMessage)
- **Conditional prompting** based on message type classification
- **Simplified approach** trusting the LLM without fallbacks
- **Comparison** between deterministic and AI-powered agents

## ğŸ” Key Differences

| Feature | non-llm-chatbot.py | llm-chatbot.py |
|---------|-------------------|----------------|
| Classification | Regex-based | LLM with structured output |
| Classification Method | Pattern matching | Pydantic schema + detailed prompts + temp 0.0 |
| Responses | Hardcoded | Generated by LLM (temp 0.7) |
| Response Customization | Dict lookup | Dynamic prompts based on message type |
| Context Awareness | None | Full conversation history (User + Bot) |
| Message History | Simple dicts | HumanMessage + AIMessage |
| Speed | Instant (~1ms) | 1-3 seconds |
| Predictability | 100% deterministic | Classification consistent, responses varied |
| Code Complexity | Simple, explicit | Simple, trusts LLM |
| Learning Focus | LangGraph workflow | LLM integration + prompt engineering |
| Dependencies | Minimal | Requires Ollama |

## ğŸš¦ Getting Started

**Recommended Learning Path:**
1. Start with `non-llm-chatbot.py` to understand LangGraph fundamentals
2. Compare the code structure and workflow (same graph, different implementations)
3. Move to `llm-chatbot.py` to see how LLMs enhance the agents
4. Observe how conversation history is maintained with `HumanMessage` + `AIMessage`
5. Experiment with different prompts, temperatures, and models
6. Notice how detailed prompts improve classification accuracy
